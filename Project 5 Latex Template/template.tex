\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{float}

\input{assignment.sty}
\begin{document}


\setassignment
\setduedate{7.12.2022, 23:59}

\serieheader{High-Performance Computing}{2022}{Student: SIMONE TARENZI}{Discussed with: FULL NAME}{Solution for Project 5}{}
\newline


\section{Task 1 - Initialize and finalize MPI [5 Points]}

Initialized MPI with MPI\_Init\_thread by passing argc, argv, and MPI\_THREAD\_FUNNELED, got the rank with MPI\_Comm\_rank and the size with MPI\_Comm\_size, and finalized with MPI\_Finalize.

\section{Task 2 - Create a Cartesian topology [10 Points]}

First I used MPI\_Dims\_create to create the number of partitions depending on mpi\_size, then I used MPI\_Cart\_create to create the communicator which will be used to communicate with the neighbouring topology. After that I used MPI\_Cart\_coords to retrieve the coordinates of each process, and finally MPI\_Cart\_shift to get the neighbours of each rank.

\section{Task 3 - Change linear algebra functions [5 Points]}

I used MPI\_Allreduce to get the reduced result of the two operations. It's only needed in those because the two functions are working on the same result between all processes, and so there must be no race conditions to get it correct.

\section{Task 4 - Exchange ghost cells [45 Points]}

Thanks to the given code for receiving and sending data to the northern neighbour, implementing the rest of the code for the other neighbours was very straighforward.
\newline
I put MPI\_Waitall after the interior grid point are calculated since they are not sent to the neighbours.

\section{Task 5 - Testing [20 Points]}

I used a simple bash script to iterate the various testing required, from 1 to 10 OMP threads.
\newline
The results were saved in their own .txt file depending on the matrix size and on the number of ranks used.
\newline
After that, I used python to plot the results into a graph for each grid size. (I'm sorry in advance for the graphs, they are not very good. The more precise results can be found in the .txt files.)

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{128x128_plot.png}
\caption{performance graph of the testing done on a 128 by 128 matrix}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{256x256_plot.png}
\caption{performance graph of the testing done on a 256 by 256 matrix}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{512x512_plot.png}
\caption{performance graph of the testing done on a 512 by 512 matrix}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{1024x1024_plot.png}
\caption{performance graph of the testing done on a 1024 by 1024 matrix}
\end{figure}
The first interesting thing to note is that increasing the number of threads doesn't improve performance when using 1 or 2 MPI ranks. This is because 

\section{Task 6 - Quality of the Report [15 Points]}



\end{document}
