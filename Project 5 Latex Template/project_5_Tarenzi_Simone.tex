\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{float}

\input{assignment.sty}
\begin{document}


\setassignment
\setduedate{7.12.2022, 23:59}

\serieheader{High-Performance Computing}{2022}{Student: SIMONE TARENZI}{Discussed with: FULL NAME}{Solution for Project 5}{}
\newline


\section{Task 1 - Initialize and finalize MPI [5 Points]}

Initialized MPI with MPI\_Init\_thread by passing argc, argv, and MPI\_THREAD\_FUNNELED, got the rank with MPI\_Comm\_rank and the size with MPI\_Comm\_size, and finalized with MPI\_Finalize.

\section{Task 2 - Create a Cartesian topology [10 Points]}

First I used MPI\_Dims\_create to create the number of partitions depending on mpi\_size, then I used MPI\_Cart\_create to create the communicator which will be used to communicate with the neighbouring topology. After that I used MPI\_Cart\_coords to retrieve the coordinates of each process, and finally MPI\_Cart\_shift to get the neighbours of each rank.

\section{Task 3 - Change linear algebra functions [5 Points]}

I used MPI\_Allreduce to get the reduced result of the two operations. It's only needed in those because the two functions are working on the same result between all processes, and so there must be no race conditions to get it correct.

\section{Task 4 - Exchange ghost cells [45 Points]}

Thanks to the given code for receiving and sending data to the northern neighbour, implementing the rest of the code for the other neighbours was very straighforward.
\newline
I put MPI\_Waitall after the interior grid point are calculated since they are not sent to the neighbours.

\section{Task 5 - Testing [20 Points]}

I used a simple bash script to iterate the various testing required, from 1 to 10 OMP threads.
\newline
The results were saved in their own .txt file depending on the matrix size and on the number of ranks used.
\newline
After that, I used python to plot the results into a graph for each grid size. (I'm sorry in advance for the graphs, they are not very good. The more precise results can be found in the .txt files.)

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{128x128_plot.png}
\caption{performance graph of the testing done on a 128 by 128 matrix}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{256x256_plot.png}
\caption{performance graph of the testing done on a 256 by 256 matrix}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{512x512_plot.png}
\caption{performance graph of the testing done on a 512 by 512 matrix}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{1024x1024_plot.png}
\caption{performance graph of the testing done on a 1024 by 1024 matrix}
\end{figure}
Some observations:
\begin{itemize}
\item Increasing the number of MPI ranks always improves performance: The bigger the matrix, the bigger the difference between them. Starting from the 256 by 256 matrix, doubling the amount of MPI ranks also doubles the performance.
\item Increasing the number of threads always worsens performance when using 1 or 2 MPI ranks. However, the loss in performance gets smaller as the matrix size increases: it's possible that with higher matrices than the ones tested, performance could improve with more threads.
\item Increasing the number of threads improves performance when using 4 MPI ranks: in the 128 by 128 matrix the best results were had when using 4 threads, in the 256 by 256 one when using 6,  in the 512 by 512 when using 9, and in the 1024 by 1024 when using 10 (the increase in performance also looks logarithmic starting from the 256 by 256 matrix).
\end{itemize}

\section{Task 6 - Quality of the Report [15 Points]}



\end{document}
